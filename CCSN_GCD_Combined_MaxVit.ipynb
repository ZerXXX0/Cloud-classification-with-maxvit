{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10537377,
          "sourceType": "datasetVersion",
          "datasetId": 6519963
        },
        {
          "sourceId": 10814352,
          "sourceType": "datasetVersion",
          "datasetId": 6713804
        },
        {
          "sourceId": 10977774,
          "sourceType": "datasetVersion",
          "datasetId": 6831242
        },
        {
          "sourceId": 10978137,
          "sourceType": "datasetVersion",
          "datasetId": 6831529
        },
        {
          "sourceId": 224331687,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "CCSN GCD Combined MaxVit",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZerXXX0/Cloud-classification-with-maxvit/blob/main/CCSN_GCD_Combined_MaxVit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "V7D86vhI25zW"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "inaiwt_gcd_dataset_path = kagglehub.dataset_download('inaiwt/gcd-dataset')\n",
        "inaiwt_ccsn_dataset_path = kagglehub.dataset_download('inaiwt/ccsn-dataset')\n",
        "inaiwt_gcd_ccsn_combined_path = kagglehub.dataset_download('inaiwt/gcd-ccsn-combined')\n",
        "inaiwt_gcd_ccsn_new_path = kagglehub.dataset_download('inaiwt/gcd-ccsn-new')\n",
        "inaiwt_maxvit_ccsn_new_path = kagglehub.notebook_output_download('inaiwt/maxvit-ccsn-new')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "WvUlohol25za"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import timm\n",
        "import torch\n",
        "import zipfile,os\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-14T20:30:21.835001Z",
          "iopub.execute_input": "2025-03-14T20:30:21.835308Z",
          "iopub.status.idle": "2025-03-14T20:30:30.998975Z",
          "shell.execute_reply.started": "2025-03-14T20:30:21.835283Z",
          "shell.execute_reply": "2025-03-14T20:30:30.998313Z"
        },
        "id": "5A_YkZJo25zb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/kaggle/input/gcd-ccsn-new/GCD CCSN Combined with val/train'\n",
        "test_dir =  '/kaggle/input/gcd-ccsn-new/GCD CCSN Combined with val/test'\n",
        "val_dir =  '/kaggle/input/gcd-ccsn-new/GCD CCSN Combined with val/val'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-14T20:30:30.999761Z",
          "iopub.execute_input": "2025-03-14T20:30:31.000175Z",
          "iopub.status.idle": "2025-03-14T20:30:31.003794Z",
          "shell.execute_reply.started": "2025-03-14T20:30:31.000146Z",
          "shell.execute_reply": "2025-03-14T20:30:31.002995Z"
        },
        "id": "OVI_q_j_25zb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = timm.create_model(\n",
        "    'maxvit_tiny_tf_224.in1k',\n",
        "    pretrained=False,\n",
        "    num_classes=7,\n",
        ")\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "# get model specific transforms (normalization, resize)\n",
        "data_config = timm.data.resolve_model_data_config(model)\n",
        "trans = timm.data.create_transform(**data_config, is_training=False)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_names = os.listdir(data_dir)\n",
        "\n",
        "        for label, class_name in enumerate(self.class_names):\n",
        "            class_dir = os.path.join(data_dir, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label  # Ensure this returns a tuple of (image, label)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.RandomHorizontalFlip(p=0.5),           # Flip horizontally (clouds are often symmetrical)\n",
        "    transforms.RandomVerticalFlip(p=0.3),             # Flip vertically (useful for aerial/satellite views)\n",
        "    transforms.RandomRotation(30),                    # Random rotations to simulate viewpoint changes\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop to add variability\n",
        "    transforms.RandomAutocontrast(),                  # Auto-contrast to normalize lighting\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2),  # Enhance sharpness for fine details\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "\n",
        "\n",
        "# Create an instance of the CustomDataset\n",
        "dataset = CustomDataset(data_dir=train_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "for data in train_loader:\n",
        "    print(data[0].shape)  # This will show you the structure of the data being returned\n",
        "    inputs, targets = data  # Unpack only if it has the correct structure\n",
        "    break\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "num_epochs = 5\n",
        "\n",
        "#New with csv added\n",
        "csv_file = \"train_log_tiny1.csv\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Time (seconds)\"])\n",
        "\n",
        "# Training loop\n",
        "model.train()  # Set the model to training mode\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "         # Calculate metrics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total * 100\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Log results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch + 1, epoch_loss, epoch_acc, epoch_time])\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "torch.save(model.state_dict(), \"model_tiny_GCD1.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-14T20:30:31.005372Z",
          "iopub.execute_input": "2025-03-14T20:30:31.005728Z",
          "iopub.status.idle": "2025-03-14T20:49:17.562283Z",
          "shell.execute_reply.started": "2025-03-14T20:30:31.005692Z",
          "shell.execute_reply": "2025-03-14T20:49:17.561426Z"
        },
        "id": "OrG8rgrD25zc",
        "outputId": "da6f188b-f64c-4516-8c3c-f9e94b2a767c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([32, 3, 224, 224])\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [1/5]: 100%|██████████| 297/297 [03:57<00:00,  1.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [1/5], Loss: 0.9746, Accuracy: 61.98%, Time: 237.85s\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [2/5]: 100%|██████████| 297/297 [03:38<00:00,  1.36it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [2/5], Loss: 0.7134, Accuracy: 72.15%, Time: 218.79s\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [3/5]: 100%|██████████| 297/297 [03:44<00:00,  1.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [3/5], Loss: 0.6226, Accuracy: 75.53%, Time: 224.48s\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [4/5]: 100%|██████████| 297/297 [03:39<00:00,  1.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [4/5], Loss: 0.5647, Accuracy: 77.93%, Time: 219.73s\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [5/5]: 100%|██████████| 297/297 [03:43<00:00,  1.33it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [5/5], Loss: 0.5067, Accuracy: 80.21%, Time: 223.67s\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "test_dataset = ImageFolder(root=test_dir, transform = transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 4. Define testing loop\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_targets = []\n",
        "all_preds = []\n",
        "\n",
        "\n",
        "# Perform forward pass\n",
        "#outputs = model(inputs)\n",
        "\n",
        "# Get predicted class indices\n",
        "#_, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "# Compare predictions with ground truth\n",
        "#correct_predictions += (preds == targets).sum().item()\n",
        "#total_samples += targets.size(0)\n",
        "\n",
        "# Store predictions and labels for metrics\n",
        "#all_targets.extend(targets.tolist())\n",
        "#all_preds.extend(preds.tolist())\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        all_targets.extend(targets.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "accuracy = accuracy_score(all_targets, all_preds)\n",
        "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "report = classification_report(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "results_file = f\"Tiny test1.txt\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
        "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
        "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(conf_matrix)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-14T20:49:17.564059Z",
          "iopub.execute_input": "2025-03-14T20:49:17.564296Z",
          "iopub.status.idle": "2025-03-14T20:50:11.003358Z",
          "shell.execute_reply.started": "2025-03-14T20:49:17.564277Z",
          "shell.execute_reply": "2025-03-14T20:50:11.002601Z"
        },
        "id": "rYT3vzSN25zd",
        "outputId": "c69b45aa-1605-47d9-b147-753dabea1105"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 6.20%\nPrecision: [0.09120521 0.03623188 0.03686636 0.0443787  0.09452736 0.02380952\n 0.36781609]\nRecall: [0.30769231 0.02262443 0.05755396 0.0094399  0.05588235 0.16115702\n 0.10543657]\n[[  56    1   11   27   22   19   46]\n [  28    5   14   42   82   27   23]\n [  11    1    8   62   41   11    5]\n [   2    0   85   15    2 1482    3]\n [ 142    7   56   49   19   59    8]\n [  91    0   24   61    2   39   25]\n [ 284  124   19   82   33    1   64]]\n<function classification_report at 0x7c73320cef80>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TQVJmzNn25zd"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}