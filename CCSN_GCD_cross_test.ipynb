{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11058392,
          "sourceType": "datasetVersion",
          "datasetId": 6889827
        },
        {
          "sourceId": 11072476,
          "sourceType": "datasetVersion",
          "datasetId": 6900171
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "CCSN GCD cross test",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZerXXX0/Cloud-classification-with-maxvit/blob/main/CCSN_GCD_cross_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "GsXUj-Ow3zj0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "inaiwt_gcd_with_val_path = kagglehub.dataset_download('inaiwt/gcd-with-val')\n",
        "inaiwt_gcd_ccsn_val_reduced_path = kagglehub.dataset_download('inaiwt/gcd-ccsn-val-reduced')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_o6Ufqzp3zj2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import timm\n",
        "import torch\n",
        "import zipfile,os\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T06:46:52.825698Z",
          "iopub.execute_input": "2025-03-19T06:46:52.826021Z",
          "iopub.status.idle": "2025-03-19T06:47:01.421347Z",
          "shell.execute_reply.started": "2025-03-19T06:46:52.825995Z",
          "shell.execute_reply": "2025-03-19T06:47:01.420454Z"
        },
        "id": "TsoyXfuj3zj3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_gcd = '/kaggle/input/gcd-ccsn-val-reduced/GCD val from Train/train'\n",
        "test_dir_gcd =  '/kaggle/input/gcd-ccsn-val-reduced/GCD val from Train/test'\n",
        "val_dir_gcd =  '/kaggle/input/gcd-ccsn-val-reduced/GCD val from Train/val'\n",
        "train_dir_ccsn = '/kaggle/input/gcd-ccsn-val-reduced/CCSN Val from Train(80_20)/train'\n",
        "test_dir_ccsn =  '/kaggle/input/gcd-ccsn-val-reduced/CCSN Val from Train(80_20)/test'\n",
        "val_dir_ccsn =  '/kaggle/input/gcd-ccsn-val-reduced/CCSN Val from Train(80_20)/val'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T06:47:01.422533Z",
          "iopub.execute_input": "2025-03-19T06:47:01.42279Z",
          "iopub.status.idle": "2025-03-19T06:47:01.426576Z",
          "shell.execute_reply.started": "2025-03-19T06:47:01.422769Z",
          "shell.execute_reply": "2025-03-19T06:47:01.425422Z"
        },
        "id": "64ictGhW3zj4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train in GCD"
      ],
      "metadata": {
        "id": "XYus_kQc3zj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained Model\n",
        "model = timm.create_model(\n",
        "    'maxvit_tiny_tf_224.in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5,\n",
        ")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Modify the classifier to include dropout\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, base_model, dropout_rate=0.3):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        in_features = base_model.head.fc.in_features  # Get original classifier input features\n",
        "        self.base_model.head.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),  # Add dropout before final layer\n",
        "            nn.Linear(in_features, 5)   # Output layer (5 classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_model(x)\n",
        "\n",
        "# Wrap the model with dropout\n",
        "dropout_model = CustomModel(model, dropout_rate=0.3)\n",
        "\n",
        "# Enable multi-GPU support if available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    dropout_model = nn.DataParallel(dropout_model)\n",
        "\n",
        "dropout_model = dropout_model.eval()\n",
        "dropout_model.to(device)\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "# get model specific transforms (normalization, resize)\n",
        "data_config = timm.data.resolve_model_data_config(model)\n",
        "trans = timm.data.create_transform(**data_config, is_training=False)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_names = os.listdir(data_dir)\n",
        "\n",
        "        for label, class_name in enumerate(self.class_names):\n",
        "            class_dir = os.path.join(data_dir, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label  # Ensure this returns a tuple of (image, label)\n",
        "\n",
        "#transform = transforms.Compose([\n",
        "#    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "#    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "#    transforms.RandomHorizontalFlip(),\n",
        "#    transforms.RandomRotation(10),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "#])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                     # Resize to match model input size\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),           # Flip horizontally (clouds are often symmetrical)\n",
        "    #transforms.RandomVerticalFlip(p=0.3),             # Flip vertically (useful for aerial/satellite views)\n",
        "    #transforms.RandomRotation(30),                    # Random rotations to simulate viewpoint changes\n",
        "    #transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop to add variability\n",
        "    #transforms.RandomAutocontrast(),                  # Auto-contrast to normalize lighting\n",
        "    #transforms.RandomAdjustSharpness(sharpness_factor=2),  # Enhance sharpness for fine details\n",
        "    transforms.ToTensor(),                            # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize for ImageNet-pretrained models\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create an instance of the CustomDataset\n",
        "dataset = CustomDataset(data_dir=train_dir_gcd, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "for data in train_loader:\n",
        "    print(data[0].shape)  # This will show you the structure of the data being returned\n",
        "    inputs, targets = data  # Unpack only if it has the correct structure\n",
        "    break\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "num_epochs = 10\n",
        "\n",
        "#New with csv added\n",
        "csv_file = \"train_log_tiny_GCD.csv\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\",\"Val Loss\", \"Val Accuracy\", \"Time (seconds)\", \"Current LR\"])\n",
        "\n",
        "val_dataset = ImageFolder(root=val_dir_gcd, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Scheduler\n",
        "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "model.train()  # Set the model to training mode\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "         # Calculate metrics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total * 100\n",
        "\n",
        "    # Validation\n",
        "    model.eval();\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_targets in val_loader:\n",
        "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_targets).item()\n",
        "            _, val_predicted = torch.max(val_outputs, 1)\n",
        "            val_total += val_targets.size(0)\n",
        "            val_correct += (val_predicted == val_targets).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct / val_total * 100\n",
        "\n",
        "    #scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Log results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch + 1, epoch_loss, epoch_acc, val_loss, val_acc, epoch_time, current_lr])\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
        "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
        "    model.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-18T09:35:16.020429Z",
          "iopub.execute_input": "2025-03-18T09:35:16.020729Z",
          "iopub.status.idle": "2025-03-18T09:41:26.277801Z",
          "shell.execute_reply.started": "2025-03-18T09:35:16.020708Z",
          "shell.execute_reply": "2025-03-18T09:41:26.276958Z"
        },
        "id": "oK6irG8C3zj6",
        "outputId": "6c0df086-fa5a-4522-b539-e5208cd51a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([32, 3, 224, 224])\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [1/10]: 100%|██████████| 23/23 [00:20<00:00,  1.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [1/10], Train Loss: 0.9412, Train Acc: 68.76%, Val Loss: 2.4692, Val Acc: 2.46%, Time: 37.70s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [2/10]: 100%|██████████| 23/23 [00:19<00:00,  1.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [2/10], Train Loss: 0.3504, Train Acc: 88.28%, Val Loss: 3.0597, Val Acc: 7.16%, Time: 36.96s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [3/10]: 100%|██████████| 23/23 [00:19<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [3/10], Train Loss: 0.2212, Train Acc: 91.77%, Val Loss: 3.7267, Val Acc: 3.55%, Time: 36.84s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [4/10]: 100%|██████████| 23/23 [00:19<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [4/10], Train Loss: 0.1212, Train Acc: 96.09%, Val Loss: 3.8519, Val Acc: 4.16%, Time: 36.88s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [5/10]: 100%|██████████| 23/23 [00:19<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [5/10], Train Loss: 0.0920, Train Acc: 97.21%, Val Loss: 4.0690, Val Acc: 5.46%, Time: 36.88s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [6/10]: 100%|██████████| 23/23 [00:19<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [6/10], Train Loss: 0.0508, Train Acc: 98.47%, Val Loss: 4.3664, Val Acc: 6.28%, Time: 36.77s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [7/10]: 100%|██████████| 23/23 [00:19<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [7/10], Train Loss: 0.0539, Train Acc: 98.05%, Val Loss: 4.5045, Val Acc: 5.39%, Time: 36.53s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [8/10]: 100%|██████████| 23/23 [00:19<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [8/10], Train Loss: 0.0366, Train Acc: 99.02%, Val Loss: 4.7671, Val Acc: 4.37%, Time: 36.85s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [9/10]: 100%|██████████| 23/23 [00:19<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [9/10], Train Loss: 0.1103, Train Acc: 96.09%, Val Loss: 4.5737, Val Acc: 7.98%, Time: 36.66s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [10/10]: 100%|██████████| 23/23 [00:19<00:00,  1.17it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [10/10], Train Loss: 0.0880, Train Acc: 97.21%, Val Loss: 4.5922, Val Acc: 5.53%, Time: 36.69s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test in CCSN"
      ],
      "metadata": {
        "id": "iWFvzo5s3zj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Resize to match the model input\n",
        "    transforms.ToTensor(),                           # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])  # Normalize (same as training)\n",
        "])\n",
        "\n",
        "test_dataset = ImageFolder(root=test_dir_ccsn, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 4. Define testing loop\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_targets = []\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        all_targets.extend(targets.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "accuracy = accuracy_score(all_targets, all_preds)\n",
        "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "report = classification_report(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "results_file = f\"Tiny train in GCD test in ccsn.txt\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
        "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
        "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(conf_matrix)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-18T09:41:26.278857Z",
          "iopub.execute_input": "2025-03-18T09:41:26.279106Z",
          "iopub.status.idle": "2025-03-18T09:41:28.976639Z",
          "shell.execute_reply.started": "2025-03-18T09:41:26.279086Z",
          "shell.execute_reply": "2025-03-18T09:41:28.97558Z"
        },
        "id": "8H-EEaCc3zj8",
        "outputId": "24002072-a682-4b47-e44d-3ab4628cf338"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 22.77%\nPrecision: [0.18181818 0.2        0.38461538 0.14285714 0.21917808]\nRecall: [0.44444444 0.04545455 0.53571429 0.02941176 0.33333333]\n[[16  0  5  0 15]\n [13  2  1 10 18]\n [ 6  1 15  1  5]\n [30  7 10  2 19]\n [23  0  8  1 16]]\n<function classification_report at 0x7e895c7ec940>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test in GCD"
      ],
      "metadata": {
        "id": "otfm-pT-3zj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Resize to match the model input\n",
        "    transforms.ToTensor(),                           # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])  # Normalize (same as training)\n",
        "])\n",
        "\n",
        "test_dataset = ImageFolder(root=test_dir_gcd, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 4. Define testing loop\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_targets = []\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        all_targets.extend(targets.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "accuracy = accuracy_score(all_targets, all_preds)\n",
        "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "report = classification_report(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "results_file = f\"Tiny train in GCD test in GCD.txt\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
        "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
        "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(conf_matrix)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "trusted": true,
        "id": "zcC0V8HD3zj9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train in CCSN"
      ],
      "metadata": {
        "id": "6P8Qg1kB3zj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained Model\n",
        "model = timm.create_model(\n",
        "    'maxvit_tiny_tf_224.in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5,\n",
        ")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Modify the classifier to include dropout\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, base_model, dropout_rate=0.3):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        in_features = base_model.head.fc.in_features  # Get original classifier input features\n",
        "        self.base_model.head.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),  # Add dropout before final layer\n",
        "            nn.Linear(in_features, 5)   # Output layer (5 classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_model(x)\n",
        "\n",
        "# Wrap the model with dropout\n",
        "dropout_model = CustomModel(model, dropout_rate=0.3)\n",
        "\n",
        "# Enable multi-GPU support if available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    dropout_model = nn.DataParallel(dropout_model)\n",
        "\n",
        "dropout_model = dropout_model.eval()\n",
        "dropout_model.to(device)\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "# get model specific transforms (normalization, resize)\n",
        "data_config = timm.data.resolve_model_data_config(model)\n",
        "trans = timm.data.create_transform(**data_config, is_training=False)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_names = os.listdir(data_dir)\n",
        "\n",
        "        for label, class_name in enumerate(self.class_names):\n",
        "            class_dir = os.path.join(data_dir, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label  # Ensure this returns a tuple of (image, label)\n",
        "\n",
        "#transform = transforms.Compose([\n",
        "#    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "#    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "#    transforms.RandomHorizontalFlip(),\n",
        "#    transforms.RandomRotation(10),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "#])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                     # Resize to match model input size\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),           # Flip horizontally (clouds are often symmetrical)\n",
        "    #transforms.RandomVerticalFlip(p=0.3),             # Flip vertically (useful for aerial/satellite views)\n",
        "    #transforms.RandomRotation(30),                    # Random rotations to simulate viewpoint changes\n",
        "    #transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop to add variability\n",
        "    #transforms.RandomAutocontrast(),                  # Auto-contrast to normalize lighting\n",
        "    #transforms.RandomAdjustSharpness(sharpness_factor=2),  # Enhance sharpness for fine details\n",
        "    transforms.ToTensor(),                            # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize for ImageNet-pretrained models\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create an instance of the CustomDataset\n",
        "dataset = CustomDataset(data_dir=train_dir_ccsn, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "for data in train_loader:\n",
        "    print(data[0].shape)  # This will show you the structure of the data being returned\n",
        "    inputs, targets = data  # Unpack only if it has the correct structure\n",
        "    break\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "num_epochs = 10\n",
        "\n",
        "#New with csv added\n",
        "csv_file = \"train_log_tiny_ccsn.csv\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\",\"Val Loss\", \"Val Accuracy\", \"Time (seconds)\", \"Current LR\"])\n",
        "\n",
        "val_dataset = ImageFolder(root=val_dir_gcd, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Scheduler\n",
        "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "model.train()  # Set the model to training mode\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "         # Calculate metrics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total * 100\n",
        "\n",
        "    # Validation\n",
        "    model.eval();\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_targets in val_loader:\n",
        "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_targets).item()\n",
        "            _, val_predicted = torch.max(val_outputs, 1)\n",
        "            val_total += val_targets.size(0)\n",
        "            val_correct += (val_predicted == val_targets).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct / val_total * 100\n",
        "\n",
        "    #scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Log results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch + 1, epoch_loss, epoch_acc, val_loss, val_acc, epoch_time, current_lr])\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
        "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
        "    model.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-18T09:41:28.97867Z",
          "iopub.execute_input": "2025-03-18T09:41:28.978914Z",
          "iopub.status.idle": "2025-03-18T09:47:31.359314Z",
          "shell.execute_reply.started": "2025-03-18T09:41:28.978894Z",
          "shell.execute_reply": "2025-03-18T09:47:31.358551Z"
        },
        "id": "36i9aObu3zkA",
        "outputId": "49291ac4-d53b-4f57-9278-57e7f8e96069"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([32, 3, 224, 224])\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [1/10]: 100%|██████████| 23/23 [00:19<00:00,  1.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [1/10], Train Loss: 1.3352, Train Acc: 47.42%, Val Loss: 1.8198, Val Acc: 13.37%, Time: 37.22s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [2/10]: 100%|██████████| 23/23 [00:19<00:00,  1.18it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [2/10], Train Loss: 0.7299, Train Acc: 76.57%, Val Loss: 1.9857, Val Acc: 20.87%, Time: 36.34s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [3/10]: 100%|██████████| 23/23 [00:19<00:00,  1.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [3/10], Train Loss: 0.4076, Train Acc: 87.31%, Val Loss: 2.1976, Val Acc: 33.29%, Time: 36.02s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [4/10]: 100%|██████████| 23/23 [00:19<00:00,  1.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [4/10], Train Loss: 0.1990, Train Acc: 95.12%, Val Loss: 2.4338, Val Acc: 23.53%, Time: 36.46s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [5/10]: 100%|██████████| 23/23 [00:19<00:00,  1.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [5/10], Train Loss: 0.1017, Train Acc: 97.63%, Val Loss: 2.9120, Val Acc: 31.24%, Time: 35.99s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [6/10]: 100%|██████████| 23/23 [00:18<00:00,  1.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [6/10], Train Loss: 0.0749, Train Acc: 98.33%, Val Loss: 2.5915, Val Acc: 27.08%, Time: 35.83s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [7/10]: 100%|██████████| 23/23 [00:19<00:00,  1.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [7/10], Train Loss: 0.0516, Train Acc: 98.88%, Val Loss: 2.8363, Val Acc: 30.08%, Time: 35.85s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [8/10]: 100%|██████████| 23/23 [00:19<00:00,  1.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [8/10], Train Loss: 0.0557, Train Acc: 98.74%, Val Loss: 3.1203, Val Acc: 21.83%, Time: 35.73s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [9/10]: 100%|██████████| 23/23 [00:19<00:00,  1.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [9/10], Train Loss: 0.0519, Train Acc: 98.19%, Val Loss: 2.8165, Val Acc: 28.04%, Time: 35.87s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [10/10]: 100%|██████████| 23/23 [00:19<00:00,  1.20it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [10/10], Train Loss: 0.0330, Train Acc: 98.61%, Val Loss: 2.6589, Val Acc: 26.74%, Time: 36.04s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test in CCSN"
      ],
      "metadata": {
        "id": "53XcLFTe3zkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Resize to match the model input\n",
        "    transforms.ToTensor(),                           # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])  # Normalize (same as training)\n",
        "])\n",
        "\n",
        "test_dataset = ImageFolder(root=test_dir_ccsn, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 4. Define testing loop\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_targets = []\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        all_targets.extend(targets.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "accuracy = accuracy_score(all_targets, all_preds)\n",
        "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "report = classification_report(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "results_file = f\"Tiny train in CCSN test in ccsn.txt\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
        "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
        "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(conf_matrix)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "trusted": true,
        "id": "OxLwLChA3zkB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test in GCD"
      ],
      "metadata": {
        "id": "lHcAc6tT3zkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Resize to match the model input\n",
        "    transforms.ToTensor(),                           # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])  # Normalize (same as training)\n",
        "])\n",
        "\n",
        "test_dataset = ImageFolder(root=test_dir_gcd, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# 4. Define testing loop\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_targets = []\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        all_targets.extend(targets.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "accuracy = accuracy_score(all_targets, all_preds)\n",
        "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "report = classification_report(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "results_file = f\"Tiny train in CCSN test in gcd.txt\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
        "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
        "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(conf_matrix)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-18T09:47:31.360732Z",
          "iopub.execute_input": "2025-03-18T09:47:31.361009Z",
          "iopub.status.idle": "2025-03-18T09:47:41.967438Z",
          "shell.execute_reply.started": "2025-03-18T09:47:31.36098Z",
          "shell.execute_reply": "2025-03-18T09:47:41.966545Z"
        },
        "id": "hc5PrG9C3zkB",
        "outputId": "d968d79a-7156-416d-f59d-3ab9e1ea3472"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 10.12%\nPrecision: [0.22222222 0.48837209 0.14285714 0.07560628 0.05714286]\nRecall: [0.05405405 0.03250774 0.20689655 0.76811594 0.04081633]\n[[  2   3   0  10  22]\n [  2  21  34 578  11]\n [  0   2   6  21   0]\n [  1  13   2  53   0]\n [  4   4   0  39   2]]\n<function classification_report at 0x7e895c7ec940>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train in both"
      ],
      "metadata": {
        "id": "VH69kSoX3zkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained Model\n",
        "model = timm.create_model(\n",
        "    'maxvit_small_tf_224.in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5,\n",
        ")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Modify the classifier to include dropout\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, base_model, dropout_rate=0.3):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        in_features = base_model.head.fc.in_features  # Get original classifier input features\n",
        "        self.base_model.head.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),  # Add dropout before final layer\n",
        "            nn.Linear(in_features, 5)   # Output layer (5 classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_model(x)\n",
        "\n",
        "# Wrap the model with dropout\n",
        "dropout_model = CustomModel(model, dropout_rate=0.3)\n",
        "\n",
        "# Enable multi-GPU support if available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    dropout_model = nn.DataParallel(dropout_model)\n",
        "\n",
        "dropout_model = dropout_model.eval()\n",
        "dropout_model.to(device)\n",
        "model = model.eval()\n",
        "\n",
        "# get model specific transforms (normalization, resize)\n",
        "data_config = timm.data.resolve_model_data_config(model)\n",
        "trans = timm.data.create_transform(**data_config, is_training=False)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_names = os.listdir(data_dir)\n",
        "\n",
        "        for label, class_name in enumerate(self.class_names):\n",
        "            class_dir = os.path.join(data_dir, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label  # Ensure this returns a tuple of (image, label)\n",
        "\n",
        "#transform = transforms.Compose([\n",
        "#    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "#    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "#    transforms.RandomHorizontalFlip(),\n",
        "#    transforms.RandomRotation(10),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "#])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                     # Resize to match model input size\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),           # Flip horizontally (clouds are often symmetrical)\n",
        "    #transforms.RandomVerticalFlip(p=0.3),             # Flip vertically (useful for aerial/satellite views)\n",
        "    #transforms.RandomRotation(30),                    # Random rotations to simulate viewpoint changes\n",
        "    #transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop to add variability\n",
        "    #transforms.RandomAutocontrast(),                  # Auto-contrast to normalize lighting\n",
        "    #transforms.RandomAdjustSharpness(sharpness_factor=2),  # Enhance sharpness for fine details\n",
        "    transforms.ToTensor(),                            # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize for ImageNet-pretrained models\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets for both sources\n",
        "dataset_1 = CustomDataset(data_dir=train_dir_gcd, transform=transform)\n",
        "dataset_2 = CustomDataset(data_dir=train_dir_ccsn, transform=transform)\n",
        "\n",
        "val_dataset_1 = ImageFolder(root=val_dir_gcd, transform=transform)\n",
        "val_dataset_2 = ImageFolder(root=val_dir_ccsn, transform=transform)\n",
        "\n",
        "dataset = torch.utils.data.ConcatDataset([dataset_1, dataset_2])\n",
        "val_dataset = torch.utils.data.ConcatDataset([val_dataset_1, val_dataset_2])\n",
        "\n",
        "\n",
        "# Create an instance of the CustomDataset\n",
        "#dataset = CustomDataset(data_dir=train_dir_gcd, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "for data in train_loader:\n",
        "    print(data[0].shape)  # This will show you the structure of the data being returned\n",
        "    inputs, targets = data  # Unpack only if it has the correct structure\n",
        "    break\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import csv\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "num_epochs = 10\n",
        "\n",
        "#New with csv added\n",
        "csv_file = \"train_log_small_GCD_ccsn_both.csv\"\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\",\"Val Loss\", \"Val Accuracy\", \"Time (seconds)\", \"Current LR\"])\n",
        "\n",
        "val_dataset = ImageFolder(root=val_dir_gcd, transform=transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Scheduler\n",
        "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "model.train()  # Set the model to training mode\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    for inputs, targets in progress_bar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "         # Calculate metrics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total * 100\n",
        "\n",
        "    # Validation\n",
        "    model.eval();\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_targets in val_loader:\n",
        "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_targets).item()\n",
        "            _, val_predicted = torch.max(val_outputs, 1)\n",
        "            val_total += val_targets.size(0)\n",
        "            val_correct += (val_predicted == val_targets).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = val_correct / val_total * 100\n",
        "\n",
        "    #scheduler.step(val_loss)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Log results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch + 1, epoch_loss, epoch_acc, val_loss, val_acc, epoch_time, current_lr])\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
        "    print(f\"Current Learning Rate: {current_lr:.6f}\")\n",
        "    model.train()\n",
        "\n",
        "torch.save(model.state_dict(), \"Train in both.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T07:24:28.721684Z",
          "iopub.execute_input": "2025-03-19T07:24:28.721965Z",
          "iopub.status.idle": "2025-03-19T07:39:08.580195Z",
          "shell.execute_reply.started": "2025-03-19T07:24:28.721941Z",
          "shell.execute_reply": "2025-03-19T07:39:08.579269Z"
        },
        "id": "nBiy-IyQ3zkC",
        "outputId": "5470fe62-4b68-4e8c-cbc5-5110f0bab4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([16, 3, 224, 224])\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [1/10]: 100%|██████████| 90/90 [01:04<00:00,  1.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [1/10], Train Loss: 0.9129, Train Acc: 68.27%, Val Loss: 3.2063, Val Acc: 13.92%, Time: 88.18s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [2/10]: 100%|██████████| 90/90 [01:03<00:00,  1.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [2/10], Train Loss: 0.3901, Train Acc: 85.70%, Val Loss: 3.9438, Val Acc: 11.60%, Time: 87.71s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [3/10]: 100%|██████████| 90/90 [01:04<00:00,  1.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [3/10], Train Loss: 0.2066, Train Acc: 93.44%, Val Loss: 4.4852, Val Acc: 9.96%, Time: 87.94s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [4/10]: 100%|██████████| 90/90 [01:03<00:00,  1.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [4/10], Train Loss: 0.1250, Train Acc: 96.23%, Val Loss: 5.0355, Val Acc: 12.07%, Time: 88.06s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [5/10]: 100%|██████████| 90/90 [01:03<00:00,  1.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [5/10], Train Loss: 0.1251, Train Acc: 96.09%, Val Loss: 5.1792, Val Acc: 13.03%, Time: 87.87s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [6/10]: 100%|██████████| 90/90 [01:03<00:00,  1.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [6/10], Train Loss: 0.0683, Train Acc: 97.56%, Val Loss: 5.5237, Val Acc: 11.05%, Time: 87.70s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [7/10]: 100%|██████████| 90/90 [01:03<00:00,  1.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [7/10], Train Loss: 0.0379, Train Acc: 98.68%, Val Loss: 5.8410, Val Acc: 12.07%, Time: 87.52s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [8/10]: 100%|██████████| 90/90 [01:03<00:00,  1.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [8/10], Train Loss: 0.0397, Train Acc: 98.81%, Val Loss: 6.1884, Val Acc: 12.01%, Time: 87.49s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [9/10]: 100%|██████████| 90/90 [01:03<00:00,  1.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [9/10], Train Loss: 0.0688, Train Acc: 97.07%, Val Loss: 5.3030, Val Acc: 15.14%, Time: 87.49s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch [10/10]: 100%|██████████| 90/90 [01:03<00:00,  1.41it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [10/10], Train Loss: 0.0781, Train Acc: 97.21%, Val Loss: 4.8328, Val Acc: 13.92%, Time: 87.62s\nCurrent Learning Rate: 0.000100\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test in GCD"
      ],
      "metadata": {
        "id": "Ua0-UrzC3zkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Resize to match the model input\n",
        "    transforms.ToTensor(),                           # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])  # Normalize (same as training)\n",
        "])\n",
        "\n",
        "test_dataset = ImageFolder(root=test_dir_gcd, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 4. Define testing loop\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_targets = []\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        all_targets.extend(targets.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "accuracy = accuracy_score(all_targets, all_preds)\n",
        "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "report = classification_report(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "results_file = f\"Small train in both test in gcd.txt\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
        "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
        "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(conf_matrix)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T07:39:08.581711Z",
          "iopub.execute_input": "2025-03-19T07:39:08.582021Z",
          "iopub.status.idle": "2025-03-19T07:39:23.496279Z",
          "shell.execute_reply.started": "2025-03-19T07:39:08.581991Z",
          "shell.execute_reply": "2025-03-19T07:39:23.495385Z"
        },
        "id": "frr58nQW3zkC",
        "outputId": "c52612ae-c869-4537-f749-4125b5b1e636"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 27.95%\nPrecision: [0.0703125  0.77865613 0.15151515 0.         0.02439024]\nRecall: [0.24324324 0.30495356 0.86206897 0.         0.02040816]\n[[  9   2   5   1  20]\n [ 72 197 115 242  20]\n [  1   3  25   0   0]\n [ 14  38  17   0   0]\n [ 32  13   3   0   1]]\n<function classification_report at 0x7c1b5f0f4940>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test in CCSN"
      ],
      "metadata": {
        "id": "NrPMr_Ls3zkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                   # Resize to match the model input\n",
        "    transforms.ToTensor(),                           # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])  # Normalize (same as training)\n",
        "])\n",
        "\n",
        "test_dataset = ImageFolder(root=test_dir_ccsn, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# 4. Define testing loop\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "all_targets = []\n",
        "all_preds = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "        all_targets.extend(targets.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "\n",
        "accuracy = accuracy_score(all_targets, all_preds)\n",
        "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
        "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "report = classification_report(all_targets, all_preds, zero_division=0)\n",
        "\n",
        "results_file = f\"Small train in both test in ccsn.txt\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
        "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
        "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
        "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(conf_matrix)\n",
        "print(classification_report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T07:39:23.49765Z",
          "iopub.execute_input": "2025-03-19T07:39:23.497978Z",
          "iopub.status.idle": "2025-03-19T07:39:27.3992Z",
          "shell.execute_reply.started": "2025-03-19T07:39:23.497947Z",
          "shell.execute_reply": "2025-03-19T07:39:27.398532Z"
        },
        "id": "_ginGLVQ3zkD",
        "outputId": "8fa447eb-b6d6-470e-efe1-be939804f63c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 22.77%\nPrecision: [0.16666667 0.11864407 0.54166667 0.20512821 0.25757576]\nRecall: [0.16666667 0.15909091 0.46428571 0.11764706 0.35416667]\n[[ 6  2  2  2 24]\n [ 1  7  2 14 20]\n [ 0  1 13 14  0]\n [ 3 47  5  8  5]\n [26  2  2  1 17]]\n<function classification_report at 0x7c1b5f0f4940>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Jhp8XNUn3zkD"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}